# Dockerfile.airflow для создания образа Airflow с поддержкой Spark и ClickHouse.
# Этот Dockerfile.airflow:
#     - Устанавливает необходимые системные утилиты (wget, default-jdk).
#     - Устанавливает Spark.
#     - Устанавливает Python зависимости из файла requirements.txt.
#     - Настраивает переменные окружения для Spark и Java.
#     - Устанавливает пользователя airflow.

FROM apache/airflow:2.9.2

USER root

# Установка необходимых пакетов
RUN apt-get update && \
    apt install -y default-jdk wget && \
    apt-get autoremove -yqq --purge && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Установка Spark
RUN wget https://archive.apache.org/dist/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz && \
    mkdir -p /opt/spark && \
    tar -xvf spark-3.4.2-bin-hadoop3.tgz -C /opt/spark && \
    rm spark-3.4.2-bin-hadoop3.tgz

# Настройка переменных окружения
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV SPARK_HOME=/opt/spark/spark-3.4.2-bin-hadoop3
ENV PATH=$PATH:$JAVA_HOME/bin:$SPARK_HOME/bin
ENV PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
ENV PATH="/home/airflow/.local/bin:${PATH}"

# Копирование и установка Python зависимостей
COPY requirements.txt /requirements.txt
RUN chmod 777 /requirements.txt

RUN echo '#!/bin/bash' > /usr/local/bin/debug-entrypoint && \
    echo 'set -e' >> /usr/local/bin/debug-entrypoint && \
    echo 'echo "PATH=$PATH"' >> /usr/local/bin/debug-entrypoint && \
    echo 'which airflow || echo "airflow not found in PATH"' >> /usr/local/bin/debug-entrypoint && \
    echo 'find /home/airflow/.local/bin -name airflow || echo "airflow not found in .local/bin"' >> /usr/local/bin/debug-entrypoint && \
    echo 'exec /entrypoint "$@"' >> /usr/local/bin/debug-entrypoint && \
    chmod +x /usr/local/bin/debug-entrypoint

USER airflow
RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r /requirements.txt


